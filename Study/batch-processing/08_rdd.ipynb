{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742d2965-58c8-42d0-b1f0-5fddadd39006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/24 17:07:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e49970b-7ff5-4c7a-8cc4-78a91e99d698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/24 17:07:55 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: data/pq/green/*/*.\n",
      "java.io.FileNotFoundException: File data/pq/green/*/* does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:980)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1301)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:970)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:305)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:57)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"data/pq/green/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3881d0a-55c0-4545-81a3-11365beea679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSELECT \\n    PULocationID AS zone,\\n    date_trunc('hour', lpep_pickup_datetime) AS hour, \\n    SUM(total_amount) AS amount,\\n    COUNT(1) as number_records\\n\\nFROM \\n    green\\nWHERE\\n    lpep_pickup_datetime >= '2020-01-01 00:00:00'\\nGROUP BY\\n    1,2\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SELECT \n",
    "    PULocationID AS zone,\n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) as number_records\n",
    "\n",
    "FROM \n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1,2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88244e20-6e4c-414a-9387-688b3f917892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf2c113-fc89-4694-9f79-d74242e265f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_green\\\n",
    "    .select('lpep_pickup_datetime','PULocationID','total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12ea90ab-e9c0-4142-a4c5-c470b2a82b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, total_amount=33.45),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), PULocationID=260, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), PULocationID=82, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), PULocationID=166, total_amount=12.74)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e3071a-5102-48c3-b63d-3f01f0e0f9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda row: False).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dcf1d3e-25c2-4b73-8b89-4606a1247bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdfee891-790b-4c80-80ea-fb82d9992dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020,month=1,day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427aa827-683b-4fd7-8cc8-7ebf8b9a7f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_outliers).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f006b061-0232-4db4-9cf5-a7709ec50d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fa7247e-4b4a-4273-a62e-adbb497aeacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, total_amount=33.45)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ee7f01-63a8-4f37-b69f-c461b2c19688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row):\n",
    "    hour = row.lpep_pickup_datetime.replace(second=0,minute=0,microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour,zone)\n",
    "\n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount,count)\n",
    "\n",
    "    return (key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2ed4739-cbf8-4b09-a9b3-f68ceefe0b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((datetime.datetime(2020, 1, 20, 15, 0), 67), (33.45, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c02ef1-49c4-45b3-bd34-6ffbfb794274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "\n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "\n",
    "    return (output_amount,output_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55df2658-0106-4a6f-b494-8d26426363a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1e959eb-ccb1-42e9-be8a-b568722ee292",
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow',['hour','zone','revenue','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "369eb4aa-48fb-4326-8aa5-a66c3895f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(hour=row[0][0],zone=row[0][1],revenue=row[1][0],count=row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08053cb-94a5-4348-8ba4-76ee8ccbecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RevenueRow(hour=datetime.datetime(2020, 1, 4, 20, 0), zone=129, revenue=583.2700000000001, count=38),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 3, 9, 0), zone=61, revenue=142.21, count=9),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 12, 12, 0), zone=82, revenue=290.41, count=14),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 28, 16, 0), zone=197, revenue=831.4399999999998, count=18),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 10, 22, 0), zone=95, revenue=407.71000000000015, count=37),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 18, 7, 0), zone=55, revenue=48.3, count=1),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 12, 15, 0), zone=82, revenue=265.7900000000001, count=29),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 31, 21, 0), zone=41, revenue=588.1600000000001, count=40),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 25, 18, 0), zone=65, revenue=457.06000000000023, count=28),\n",
       " RevenueRow(hour=datetime.datetime(2020, 1, 26, 14, 0), zone=166, revenue=301.7900000000001, count=26)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670ed623-617e-4862-9138-59e6a02e73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb6e4c7-e81f-4188-9e28-9445981b958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True), \n",
    "    types.StructField('zone', types.IntegerType(), True), \n",
    "    types.StructField('revenue', types.DoubleType(), True), \n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa912f6-da80-4fd0-811a-7f3d92add8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74377a43-6487-415f-ae93-1afa432a62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==================================================>       (7 + 1) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-04 20:00:00| 129| 583.2700000000001|   38|\n",
      "|2020-01-03 09:00:00|  61|            142.21|    9|\n",
      "|2020-01-12 12:00:00|  82|            290.41|   14|\n",
      "|2020-01-28 16:00:00| 197| 831.4399999999998|   18|\n",
      "|2020-01-10 22:00:00|  95|407.71000000000015|   37|\n",
      "|2020-01-18 07:00:00|  55|              48.3|    1|\n",
      "|2020-01-12 15:00:00|  82| 265.7900000000001|   29|\n",
      "|2020-01-31 21:00:00|  41| 588.1600000000001|   40|\n",
      "|2020-01-25 18:00:00|  65|457.06000000000023|   28|\n",
      "|2020-01-26 14:00:00| 166| 301.7900000000001|   26|\n",
      "|2020-01-28 15:00:00|  75|1745.2499999999984|   99|\n",
      "|2020-01-23 12:00:00| 119|             51.89|    2|\n",
      "|2020-01-17 15:00:00| 145|             96.25|    3|\n",
      "|2020-01-12 00:00:00| 127|              17.8|    1|\n",
      "|2020-01-24 16:00:00|  74|1072.2699999999995|   62|\n",
      "|2020-01-04 10:00:00| 212|             81.67|    4|\n",
      "|2020-01-03 10:00:00| 177|            201.99|    6|\n",
      "|2020-01-15 22:00:00| 260|            154.49|   11|\n",
      "|2020-01-21 19:00:00| 255|            130.77|   14|\n",
      "|2020-01-25 11:00:00|  65|253.39000000000001|   20|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/opt/homebrew/Cellar/apache-spark/4.1.0/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/4.1.0/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
      "    outfile.flush()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4882615-1ade-4bb4-bd64-f68c19b68f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 22:42:42 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22a8f028-63f7-47e8-beca-39e63f929ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID','lpep_pickup_datetime','PULocationID','DOLocationID','trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a1e5191-3cac-417e-9a18-63f78346d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, DOLocationID=130, trip_distance=12.77),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, DOLocationID=39, trip_distance=8.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), PULocationID=260, DOLocationID=157, trip_distance=1.27),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), PULocationID=82, DOLocationID=83, trip_distance=1.25),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), PULocationID=166, DOLocationID=42, trip_distance=1.84)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e833f71f-1380-44c1-85b6-1ddb22248a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(partition):\n",
    "    return [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2f526f5-0aba-4cdf-ba93-a2ae1d238496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.mapPartitions(apply_model_in_batch).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cac14-0b2d-41b6-a861-8551d3a05c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
